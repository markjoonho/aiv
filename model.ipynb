{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"./ckpt/20250313_172710/best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from clip_dataset import ImageTextDataset, collate_fn  # 사용자 정의 데이터셋 모듈\n",
    "from loss import CLIPContrastiveLoss              # 사용자 정의 손실함수\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "class OWLVITCLIPModel:\n",
    "    \"\"\"\n",
    "    OwlViT 모델을 로드하고, LoRA를 적용한 후 head만 학습할 수 있도록 하는 클래스입니다.\n",
    "    여기서는 bbox 예측 head(box_head)와 클래스 예측 head(class_head)만 학습합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"google/owlvit-base-patch32\", device='cuda', use_lora=True, lora_config_params=None):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "        # 프로세서 및 기본 모델 로드\n",
    "        self.processor = OwlViTProcessor.from_pretrained(model_name)\n",
    "        self.model = OwlViTForObjectDetection.from_pretrained(model_name).to(self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        # 전체 파라미터 Freeze\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if use_lora:\n",
    "            # 기본 LoRA 하이퍼파라미터 값 (필요시 조정)\n",
    "            if lora_config_params is None:\n",
    "                lora_config_params = {\"r\": 4, \"lora_alpha\": 32, \"lora_dropout\": 0.1}\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=\"OTHER\",  # 태스크에 따라 적절한 task_type으로 변경 가능\n",
    "                r=lora_config_params[\"r\"],\n",
    "                lora_alpha=lora_config_params[\"lora_alpha\"],\n",
    "                lora_dropout=lora_config_params[\"lora_dropout\"],\n",
    "                target_modules=[\"text_projection\", \"visual_projection\"]\n",
    "            )\n",
    "            # PEFT 라이브러리를 이용하여 LoRA 어댑터 추가\n",
    "            self.model = get_peft_model(self.model, lora_config)\n",
    "        else:\n",
    "            # LoRA를 사용하지 않는 경우, 예시로 text_projection, visual_projection만 unfreeze\n",
    "            trainable_layers = [\n",
    "                self.model.owlvit.text_projection,\n",
    "                self.model.owlvit.visual_projection\n",
    "            ]\n",
    "            for layer in trainable_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "            self.model.owlvit.logit_scale.requires_grad = True\n",
    "\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        logging.info(f\"🚀 초기 trainable 파라미터: {trainable_params / 1e6:.2f}M\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"\n",
    "        checkpoint에서 모델 state_dict를 로드합니다.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        logging.info(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "\n",
    "    def freeze_except_heads(self):\n",
    "        \"\"\"\n",
    "        모델의 모든 파라미터를 freeze하고, 'box_head'와 'class_head'에 해당하는 파라미터만 학습 가능하도록 설정합니다.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"box_head\" in name or \"class_head\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        logging.info(f\"🚀 Head만 학습 가능하도록 설정됨. Trainable 파라미터: {trainable_params / 1e6:.2f}M\")\n",
    "\n",
    "    def reinitialize_heads(self):\n",
    "        \"\"\"\n",
    "        box_head와 class_head에 해당하는 모듈들의 파라미터를 재초기화합니다.\n",
    "        \"\"\"\n",
    "        def _reinit_module(module, module_name):\n",
    "            if hasattr(module, \"reset_parameters\"):\n",
    "                module.reset_parameters()\n",
    "                logging.info(f\"{module_name} 재초기화됨.\")\n",
    "        for name, module in self.model.named_modules():\n",
    "            if \"box_head\" in name or \"class_head\" in name:\n",
    "                _reinit_module(module, name)\n",
    "\n",
    "    def get_optimizer(self, lr=1e-4):\n",
    "        \"\"\"학습 가능한 파라미터(여기서는 head만)를 업데이트하는 옵티마이저 반환\"\"\"\n",
    "        return optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr)\n",
    "\n",
    "    def get_dataloaders(self, train_dir, val_dir, batch_size=16):\n",
    "        \"\"\"데이터 로더 생성\"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        ])\n",
    "\n",
    "        train_dataset = ImageTextDataset(train_dir, self.processor, transform=transform)\n",
    "        val_dataset = ImageTextDataset(val_dir, self.processor)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train(self, train_dir, val_dir, epochs=10, batch_size=16, lr=1e-4, ckpt_base_dir=\"ckpt\"):\n",
    "        \"\"\"\n",
    "        학습 및 검증 루프.\n",
    "        학습 전에 freeze_except_heads()를 호출하여 head만 학습하도록 합니다.\n",
    "        \"\"\"\n",
    "        # head만 학습할 수 있도록 설정\n",
    "        self.freeze_except_heads()\n",
    "\n",
    "        train_loader, val_loader = self.get_dataloaders(train_dir, val_dir, batch_size)\n",
    "        optimizer = self.get_optimizer(lr)\n",
    "        criterion = CLIPContrastiveLoss().to(self.device)\n",
    "\n",
    "        # 체크포인트 저장 폴더 생성\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        ckpt_dir = os.path.join(ckpt_base_dir, timestamp)\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                # bbox 및 class head 학습은 보통 image_embeds, text_embeds가 아닌\n",
    "                # box_head와 class_head의 출력을 이용합니다.\n",
    "                # (아래 예시는 단순히 기존 임베딩 대비 손실을 계산하는 예시이며,\n",
    "                # 실제 bbox 및 class head 학습에는 적절한 손실 함수 및 전처리가 필요합니다.)\n",
    "                vision_embeds = outputs.image_embeds.mean(dim=(1, 2))\n",
    "                text_embeds = outputs.text_embeds.squeeze(1)\n",
    "                vision_embeds = self.model.owlvit.visual_projection(vision_embeds)\n",
    "                text_embeds = self.model.owlvit.text_projection(text_embeds)\n",
    "\n",
    "                loss = criterion(vision_embeds, text_embeds)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            avg_val_loss = self.validate(val_loader, criterion)\n",
    "            logging.info(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "            # 체크포인트 저장\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss\n",
    "            }\n",
    "            ckpt_path = os.path.join(ckpt_dir, f\"epoch_{epoch+1}.pth\")\n",
    "            torch.save(checkpoint, ckpt_path)\n",
    "            logging.info(f\"Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_ckpt_path = os.path.join(ckpt_dir, \"best_model.pth\")\n",
    "                torch.save(checkpoint, best_ckpt_path)\n",
    "                logging.info(f\"Best model updated: {best_ckpt_path}\")\n",
    "\n",
    "    def validate(self, val_loader, criterion):\n",
    "        \"\"\"검증 루프\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                vision_embeds = outputs.image_embeds.mean(dim=(1, 2))\n",
    "                text_embeds = outputs.text_embeds.squeeze(1)\n",
    "                vision_embeds = self.model.owlvit.visual_projection(vision_embeds)\n",
    "                text_embeds = self.model.owlvit.text_projection(text_embeds)\n",
    "\n",
    "                loss = criterion(vision_embeds, text_embeds)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터셋 경로 (프로젝트에 맞게 수정)\n",
    "    train_dataset_dir = \"./total_dataset/train_dataset/\"\n",
    "    val_dataset_dir = \"./total_dataset/val/\"\n",
    "\n",
    "    # 모델 인스턴스 생성 (LoRA 적용)\n",
    "    model_wrapper = OWLVITCLIPModel(use_lora=True)\n",
    "\n",
    "    # 기존 checkpoint에서 모델 로드 (원한다면 head 재초기화도 수행)\n",
    "    checkpoint_path = \"./ckpt/20250313_172710/best_model.pth\"\n",
    "    model_wrapper.load_checkpoint(checkpoint_path)\n",
    "    # (원하는 경우) head 재초기화\n",
    "    # model_wrapper.reinitialize_heads()\n",
    "\n",
    "    # head만 학습하도록 설정한 후 학습 시작\n",
    "    model_wrapper.train(\n",
    "        train_dir=train_dataset_dir,\n",
    "        val_dir=val_dataset_dir,\n",
    "        epochs=10,\n",
    "        batch_size=16,\n",
    "        lr=1e-4,\n",
    "        ckpt_base_dir=\"ckpt\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 18:37:16,478 - INFO - 🚀 Trainable Parameters: 0.01M\n"
     ]
    }
   ],
   "source": [
    "model = OWLVITCLIPModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OwlViTForObjectDetection(\n",
       "      (owlvit): OwlViTModel(\n",
       "        (text_model): OwlViTTextTransformer(\n",
       "          (embeddings): OwlViTTextEmbeddings(\n",
       "            (token_embedding): Embedding(49408, 512)\n",
       "            (position_embedding): Embedding(16, 512)\n",
       "          )\n",
       "          (encoder): OwlViTEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-11): 12 x OwlViTEncoderLayer(\n",
       "                (self_attn): OwlViTAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): OwlViTMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (vision_model): OwlViTVisionTransformer(\n",
       "          (embeddings): OwlViTVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "            (position_embedding): Embedding(577, 768)\n",
       "          )\n",
       "          (pre_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): OwlViTEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-11): 12 x OwlViTEncoderLayer(\n",
       "                (self_attn): OwlViTAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): OwlViTMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (visual_projection): lora.Linear(\n",
       "          (base_layer): Linear(in_features=768, out_features=512, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (text_projection): lora.Linear(\n",
       "          (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (class_head): OwlViTClassPredictionHead(\n",
       "        (dense0): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (logit_shift): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (logit_scale): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "      )\n",
       "      (box_head): OwlViTBoxPredictionHead(\n",
       "        (dense0): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dense1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (dense2): Linear(in_features=768, out_features=4, bias=True)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.20 ('py38_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0e5413880e9d5a5e633f786f0b1cbaf87005dea0ec54f3e8eb92b5684f13c71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
